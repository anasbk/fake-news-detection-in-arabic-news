{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import nltk\n",
    "'''\n",
    "# gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('cc.ar.300.bin')\n",
    "\n",
    "\n",
    "computer_vec = model.wv['كلب']\n",
    "\n",
    "print(computer_vec)\n",
    "\n",
    "computer_vec = model.wv['قط']\n",
    "\n",
    "print(computer_vec)\n",
    "\n",
    "print(model.most_similar(\"بيئة\"))\n",
    "\n",
    "\n",
    "'''\n",
    "def removeDiacretics(news_list):\n",
    "\n",
    "    arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    temp_list = list()\n",
    "    for news in news_list:\n",
    "        text = re.sub(arabic_diacritics, '', news)\n",
    "        temp_list.append(text)\n",
    "\n",
    "    return temp_list\n",
    "\n",
    "\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "\n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) ->\n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans(' ', ' ', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "'''\n",
    "word_tokens = word_tokenize(data)\n",
    "\n",
    "print(word_tokens)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "\n",
    "print(filtered_sentence)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def removeDiacretics(news_list):\n",
    "\n",
    "    arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    temp_list = list()\n",
    "    for news in news_list:\n",
    "\n",
    "        text = re.sub(arabic_diacritics, '', news)\n",
    "\n",
    "        temp_list.append(text)\n",
    "\n",
    "    return temp_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalizeArabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "'''\n",
    "fake = []\n",
    "for folder in os.listdir(\"FakeNews/data\"):\n",
    "    for file in os.listdir(\"FakeNews/data/\"+str(folder)):\n",
    "\n",
    "        with open('FakeNews/data/'+folder+\"/\"+file, \"r\", encoding=\"utf8\") as f:\n",
    "            txt = f.read().rstrip()\n",
    "            print(file)\n",
    "            fake.append((txt,folder))\n",
    "\n",
    "            #preprocessing\n",
    "\n",
    "df = pd.DataFrame(fake)\n",
    "print(df.shape)\n",
    "\n",
    "df.to_csv(\"dataset.csv\", sep=',',index=False)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "ar_stemmer = SnowballStemmer(\"arabic\")\n",
    "\n",
    "trainDF1 = pd.read_csv(\"dataset.csv\", delimiter=\",\", encoding='utf-8')\n",
    "trainDF2 = pd.read_csv(\"dataset2.csv\", delimiter=\",\", encoding='utf-8')\n",
    "\n",
    "trainDF = pd.read_csv(\"dataset.csv\", delimiter=\",\", encoding='utf-8')\n",
    "\n",
    "trainDF = pd.concat([trainDF1,trainDF2])\n",
    "\n",
    "print(trainDF.loc[trainDF['label'] == 'trusted'].shape)\n",
    "print(trainDF.loc[trainDF['label'] == 'untrusted'].shape)\n",
    "\n",
    "def normalizeDF(df):\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        text = df.iloc[index].text\n",
    "        if type(text) == str:\n",
    "\n",
    "            text = removeDiacretics([text])\n",
    "\n",
    "            word_tokens = word_tokenize(text[0])\n",
    "\n",
    "            filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "            stemmed_words = [ar_stemmer.stem(word) for word in filtered_sentence]\n",
    "\n",
    "            stemmed_sentence = ' '.join(stemmed_words)\n",
    "\n",
    "            df.iloc[index].text = stemmed_sentence\n",
    "\n",
    "    return df\n",
    "\n",
    "filtered_df = normalizeDF(trainDF)\n",
    "\n",
    "print(filtered_df.text.values)\n",
    "filtered_df.to_csv('to_train.csv', sep=',')\n",
    "\n",
    "print(\"filtered\")\n",
    "\n",
    "Y = pd.get_dummies(trainDF['label']).values\n",
    "\n",
    "\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(trainDF['text'], Y)\n",
    "train_x, test_x, train_y, test_y = train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "\n",
    "print(train_x)\n",
    "\n",
    "### Create sequence\n",
    "vocabulary_size = 100\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(str(e) for e in train_x)\n",
    "sequences = tokenizer.texts_to_sequences(str(e) for e in train_x)\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "\n",
    "'''\n",
    "# label encode the target variable\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "print(train_y.shape)\n",
    "test_y = encoder.fit_transform(test_y)\n",
    "'''\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_fatures = 500\n",
    "\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',max_features=300)\n",
    "tfidf_vect.fit(trainDF['text'].values.astype('U'))\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x.values.astype('U')).toarray()\n",
    "xvalid_tfidf = tfidf_vect.transform(test_x.values.astype('U')).toarray()\n",
    "\n",
    "print(xtrain_tfidf.shape)\n",
    "print(xvalid_tfidf.shape)\n",
    "\n",
    "\n",
    "\n",
    "# counter\n",
    "\n",
    "count_vec = CountVectorizer(analyzer='word', max_features=300)\n",
    "count_train = count_vec.fit(trainDF['text'].values.astype('U'))\n",
    "bag_of_words_train = count_vec.transform(train_x.values.astype('U')).toarray()\n",
    "bag_of_words_test = count_vec.transform(train_x.values.astype('U')).toarray()\n",
    "\n",
    "print(bag_of_words_train.shape)\n",
    "print(bag_of_words_test.shape)\n",
    "\n",
    "Y = pd.get_dummies(train_y)\n",
    "\n",
    "# Get embeddings from fasttext\n",
    "embeddings_index = dict()\n",
    "\n",
    "# Create a weight matrix for words in training docs\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "\n",
    "    print(index)\n",
    "    print(word)\n",
    "\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            embedding_vector = np.arange(0,300)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "        except:\n",
    "            print('exception')\n",
    "\n",
    "\n",
    "## create model\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 300,\n",
    "                          input_length=50,\n",
    "                          weights=[embedding_matrix],\n",
    "                          trainable=False))\n",
    "model_glove.add(Conv1D(96, 5, activation='relu'))\n",
    "model_glove.add(Dropout(0.5))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(64))\n",
    "model_glove.add(Dropout(0.5))\n",
    "model_glove.add(LSTM(64))\n",
    "model_glove.add(Dropout(0.5))\n",
    "model_glove.add(LSTM(64))\n",
    "model_glove.add(Dropout(0.5))\n",
    "model_glove.add(Dense(2, activation='sigmoid'))\n",
    "model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "\n",
    "model_glove.summary()\n",
    "model_glove.fit(data, Y, validation_split=0.2, epochs = 10)\n",
    "\n",
    "# Save model\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "1\n",
    "128 - lstm 100\n",
    "'''\n",
    "model_glove.save('wembedlstm.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
