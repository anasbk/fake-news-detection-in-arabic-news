{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "(0, 2)\n",
      "filtered\n",
      "(7398,) (7398,)\n",
      "(2466,) (2466,)\n",
      "(7398, 5000)\n",
      "(2466, 5000)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "import nltk\n",
    "\n",
    "def removeDiacretics(news_list):\n",
    "\n",
    "    arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    temp_list = list()\n",
    "    for news in news_list:\n",
    "        text = re.sub(arabic_diacritics, '', news)\n",
    "        temp_list.append(text)\n",
    "\n",
    "    return temp_list\n",
    "\n",
    "\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "\n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) ->\n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans(' ', ' ', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "'''\n",
    "word_tokens = word_tokenize(data)\n",
    "\n",
    "print(word_tokens)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "\n",
    "print(filtered_sentence)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def removeDiacretics(news_list):\n",
    "\n",
    "    arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    temp_list = list()\n",
    "    for news in news_list:\n",
    "\n",
    "        text = re.sub(arabic_diacritics, '', news)\n",
    "\n",
    "        temp_list.append(text)\n",
    "\n",
    "    return temp_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalizeArabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "'''\n",
    "fake = []\n",
    "for folder in os.listdir(\"FakeNews/data\"):\n",
    "    for file in os.listdir(\"FakeNews/data/\"+str(folder)):\n",
    "\n",
    "        with open('FakeNews/data/'+folder+\"/\"+file, \"r\", encoding=\"utf8\") as f:\n",
    "            txt = f.read().rstrip()\n",
    "            print(file)\n",
    "            fake.append((txt,folder))\n",
    "\n",
    "            #preprocessing\n",
    "\n",
    "df = pd.DataFrame(fake)\n",
    "print(df.shape)\n",
    "\n",
    "df.to_csv(\"dataset.csv\", sep=',',index=False)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "ar_stemmer = SnowballStemmer(\"arabic\")\n",
    "\n",
    "trainDF1 = pd.read_csv(\"dataset.csv\", delimiter=\",\", encoding='utf-8')\n",
    "trainDF2 = pd.read_csv(\"dataset2.csv\", delimiter=\",\", encoding='utf-8')\n",
    "\n",
    "trainDF = pd.read_csv(\"dataset.csv\", delimiter=\",\", encoding='utf-8')\n",
    "\n",
    "trainDF = pd.concat([trainDF1,trainDF2])\n",
    "\n",
    "print(trainDF.loc[trainDF['label'] == 'trusted'].shape)\n",
    "print(trainDF.loc[trainDF['label'] == 'untrusted'].shape)\n",
    "\n",
    "def normalizeDF(df):\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        text = df.iloc[index].text\n",
    "        if type(text) == str:\n",
    "\n",
    "            text = removeDiacretics([text])\n",
    "\n",
    "            word_tokens = word_tokenize(text[0])\n",
    "\n",
    "            filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "            stemmed_words = [ar_stemmer.stem(word) for word in filtered_sentence]\n",
    "\n",
    "            stemmed_sentence = ' '.join(stemmed_words)\n",
    "\n",
    "            df.iloc[index].text = stemmed_sentence\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_df = normalizeDF(trainDF)\n",
    "\n",
    "filtered_df.to_csv('to_train.csv', sep=',')\n",
    "\n",
    "print(\"filtered\")\n",
    "\n",
    "Y = pd.get_dummies(trainDF['label']).values\n",
    "\n",
    "\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(trainDF['text'], Y)\n",
    "train_x, test_x, train_y, test_y = train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "'''\n",
    "# label encode the target variable\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "print(train_y.shape)\n",
    "test_y = encoder.fit_transform(test_y)\n",
    "'''\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "max_fatures = 500\n",
    "\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'].values.astype('U'))\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x.values.astype('U')).toarray()\n",
    "xvalid_tfidf = tfidf_vect.transform(test_x.values.astype('U')).toarray()\n",
    "\n",
    "print(xtrain_tfidf.shape)\n",
    "print(xvalid_tfidf.shape)\n",
    "\n",
    "\n",
    "\n",
    "# counter\n",
    "\n",
    "count_vec = CountVectorizer(analyzer='word', max_features=5000)\n",
    "count_train = count_vec.fit(trainDF['text'].values.astype('U'))\n",
    "bag_of_words_train = count_vec.transform(train_x.values.astype('U')).toarray()\n",
    "bag_of_words_test = count_vec.transform(train_x.values.astype('U')).toarray()\n",
    "\n",
    "\n",
    "\n",
    "print(bag_of_words_train.shape)\n",
    "print(bag_of_words_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Y = pd.get_dummies(train_y)\n",
    "\n",
    "\n",
    "embed_dim = 64\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = xtrain_tfidf.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(xtrain_tfidf, Y, epochs=7, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import keras\n",
    "\n",
    "Y = pd.get_dummies(train_y)\n",
    "Y_t = pd.get_dummies(test_y)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(5000,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "cnnhistory=model.fit(bag_of_words_train.reshape((7398,5000,1)), Y, batch_size=32, epochs=20, validation_split=0.20)\n",
    "\n",
    "model.save('cnnmodel.h5')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
